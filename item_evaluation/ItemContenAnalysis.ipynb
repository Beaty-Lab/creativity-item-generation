{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aml7990/miniconda3/envs/AIG-CUDA-12.0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Among high and low quality items, try to identify useful patterns in text complexity metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from evaluate import load\n",
    "from readability import Readability\n",
    "# CPS\n",
    "train = pd.read_csv(\"/home/aml7990/Code/creativity-item-generation/item_evaluation/cleaned_datasets/CPS_train.csv\")\n",
    "test = pd.read_csv(\"/home/aml7990/Code/creativity-item-generation/item_evaluation/cleaned_datasets/CPS_test.csv\")\n",
    "df = pd.concat([train, test])\n",
    "\n",
    "# Bloom Social Science items\n",
    "# df = pd.read_json(\"/home/aml7990/Code/creativity-item-generation/item_evaluation/cleaned_datasets/AEQG-SocialSciences-Bloom.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creative_scenario_round_4</th>\n",
       "      <th>complexity_aggregrate</th>\n",
       "      <th>difficulty_aggregrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noah is a high school student who loves to gar...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           creative_scenario_round_4  complexity_aggregrate  \\\n",
       "0  Noah is a high school student who loves to gar...                      3   \n",
       "\n",
       "   difficulty_aggregrate  \n",
       "0                      2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomsSkill\n",
       "2    95\n",
       "1    88\n",
       "4    87\n",
       "5    70\n",
       "6    65\n",
       "3    51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"BloomsSkill\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "readability_tokenizer= AutoTokenizer.from_pretrained(\"agentlans/deberta-v3-xsmall-readability-v2\")\n",
    "readability_model = AutoModelForSequenceClassification.from_pretrained(\"agentlans/deberta-v3-xsmall-readability-v2\").to(\"cuda:1\")\n",
    "def readability(text):\n",
    "    \"\"\"Processes the text using the model and returns its logits.\n",
    "    In this case, it's reading grade level in years of education\n",
    "    (the higher the number, the harder it is to read the text).\"\"\"\n",
    "    inputs = readability_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda:1\")\n",
    "    with torch.no_grad():\n",
    "        logits = readability_model(**inputs).logits.squeeze().cpu()\n",
    "    return logits.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from the official hf implementation: https://huggingface.co/spaces/evaluate-metric/perplexity/blob/main/perplexity.py\n",
    "from evaluate import logging\n",
    "from torch.nn import CrossEntropyLoss\n",
    "def compute_perplexity(\n",
    "        predictions, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "    ):\n",
    "\n",
    "        if device is not None:\n",
    "            assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "            if device == \"gpu\":\n",
    "                device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # if batch_size > 1 (which generally leads to padding being required), and\n",
    "        # if there is not an already assigned pad_token, assign an existing\n",
    "        # special token to also be the padding token\n",
    "        if tokenizer.pad_token is None and batch_size > 1:\n",
    "            existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "            # check that the model already has at least one special token defined\n",
    "            assert (\n",
    "                len(existing_special_tokens) > 0\n",
    "            ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "            # assign one of the special tokens to also be the pad token\n",
    "            tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "        if add_start_token and max_length:\n",
    "            # leave room for <BOS> token to be added:\n",
    "            assert (\n",
    "                tokenizer.bos_token is not None\n",
    "            ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "            max_tokenized_len = max_length - 1\n",
    "        else:\n",
    "            max_tokenized_len = max_length\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            predictions,\n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True if max_tokenized_len else False,\n",
    "            max_length=max_tokenized_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        encoded_texts = encodings[\"input_ids\"]\n",
    "        attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "        # check that each input is long enough:\n",
    "        if add_start_token:\n",
    "            assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "        else:\n",
    "            assert torch.all(\n",
    "                torch.ge(attn_masks.sum(1), 2)\n",
    "            ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "        ppls = []\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        for start_index in logging.tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "            end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "            encoded_batch = encoded_texts[start_index:end_index]\n",
    "            attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "            if add_start_token:\n",
    "                bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "                encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "                attn_mask = torch.cat(\n",
    "                    [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "                )\n",
    "\n",
    "            labels = encoded_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "            perplexity_batch = torch.exp(\n",
    "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                / shift_attention_mask_batch.sum(1)\n",
    "            )\n",
    "\n",
    "            ppls += perplexity_batch.tolist()\n",
    "\n",
    "        return {\"perplexities\": ppls, \"mean_perplexity\": np.mean(ppls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# define the metrics we'll use\n",
    "# perplexity\n",
    "df[\"perplexity\"] = df[\"creative_scenario_round_4\"].apply(lambda x: compute_perplexity(predictions=[x], model=model, tokenizer=tokenizer)[\"mean_perplexity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burstiness\n",
    "# implementation taken from: https://medium.com/@jhanwarsid/human-contentanalysing-perplexity-and-burstiness-in-ai-vs-human-text-df70fdcc5525\n",
    "\n",
    "def calculate_burstiness(text):\n",
    "    sentences = text.split('. ')\n",
    "    sentence_lengths = [len(tokenizer.encode(sentence)) for sentence in sentences if sentence]\n",
    "    burstiness = np.std(sentence_lengths)\n",
    "    return burstiness\n",
    "\n",
    "\n",
    "df[\"burstiness\"] = df[\"creative_scenario_round_4\"].apply(lambda x: calculate_burstiness(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fano factor, similar to burstiness but captures both differnces in mean and variance\n",
    "# implementation taken from: https://medium.com/@jhanwarsid/human-contentanalysing-perplexity-and-burstiness-in-ai-vs-human-text-df70fdcc5525\n",
    "\n",
    "def calculate_fano_factor(text):\n",
    "    sentences = text.split('. ')\n",
    "    sentence_lengths = [len(tokenizer.encode(sentence)) for sentence in sentences if sentence]\n",
    "    mean_length = np.mean(sentence_lengths)\n",
    "    variance = np.var(sentence_lengths)\n",
    "    fano_factor = variance / mean_length if mean_length > 0 else 0\n",
    "    return fano_factor\n",
    "\n",
    "\n",
    "df[\"fano_factor\"] = df[\"creative_scenario_round_4\"].apply(lambda x: calculate_fano_factor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated readability index (doesn't work for the bloom dataset)\n",
    "df[\"ARI\"] = df[\"creative_scenario_round_4\"].apply(lambda x: Readability(x).ari().score)\n",
    "# df[\"readability\"] = df[\"creative_scenario_round_4\"].apply(lambda x: readability(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token type ratio\n",
    "# implementation from: https://medium.com/@rajeswaridepala/empirical-laws-ttr-cc9f826d304d\n",
    "# I have tweaked it slightly to better handle punctuation\n",
    "def calculate_ttr(text):\n",
    "\n",
    "    # tokens are nothing but words in the sentence\n",
    "    text = text.replace(\".\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\",\", \"\")\n",
    "    words = text.split()\n",
    "\n",
    "    # type\n",
    "    unique_words = set(words)\n",
    "\n",
    "    # The TTR value is always between 0 and 1, where higher values indicate greater lexical diversity.\n",
    "    ttr = len(unique_words) / len(words)\n",
    "\n",
    "    return ttr\n",
    "\n",
    "df[\"ttr\"] = df[\"creative_scenario_round_4\"].apply(lambda x: calculate_ttr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(100.0000), tensor(95.3724), tensor(95.3592), tensor(95.0047), tensor(94.9642), tensor(95.4893), tensor(95.7182), tensor(95.1296), tensor(95.0241), tensor(96.2331), tensor(94.8144), tensor(95.3802), tensor(94.8896), tensor(95.8391), tensor(95.7969), tensor(94.7606), tensor(95.2712), tensor(95.0924), tensor(95.3448), tensor(95.3293), tensor(95.5455), tensor(95.5215), tensor(95.3816), tensor(95.2899), tensor(95.4145), tensor(93.8860), tensor(95.2848), tensor(94.5221), tensor(94.7435), tensor(95.3309), tensor(94.8927), tensor(95.6405), tensor(95.4657), tensor(95.5966), tensor(94.9863), tensor(93.9462), tensor(95.1080), tensor(93.8950), tensor(94.3849), tensor(94.9762), tensor(95.1565), tensor(94.1645), tensor(95.2614), tensor(95.5566), tensor(94.4896), tensor(94.8634), tensor(94.8949), tensor(94.7370), tensor(95.6271), tensor(95.0108), tensor(95.3081), tensor(94.9933), tensor(94.5662), tensor(94.7024), tensor(95.3438), tensor(94.7369), tensor(94.4229), tensor(94.8200), tensor(95.6269), tensor(95.5275), tensor(94.4092), tensor(94.6421), tensor(94.7941), tensor(95.0692), tensor(94.0500), tensor(94.4859), tensor(94.9246), tensor(95.4313), tensor(94.5931), tensor(95.1958), tensor(94.7294), tensor(95.5210), tensor(95.2136), tensor(95.0465), tensor(93.9449), tensor(95.2203), tensor(95.3874), tensor(94.3036), tensor(94.8767), tensor(94.7457), tensor(94.9446), tensor(95.2694), tensor(94.4573), tensor(95.3304), tensor(94.0416), tensor(95.5578), tensor(95.3738), tensor(94.7615), tensor(94.5300), tensor(95.1953), tensor(95.4013), tensor(94.8139), tensor(94.2333), tensor(94.7711), tensor(96.0522), tensor(95.1303), tensor(94.7513), tensor(95.2810), tensor(95.2719), tensor(94.6114), tensor(94.7744), tensor(95.4579), tensor(94.3433), tensor(94.9495), tensor(97.4065), tensor(95.5295), tensor(95.0910), tensor(94.8681), tensor(93.8175), tensor(94.1983), tensor(94.4591), tensor(95.5238), tensor(95.2630), tensor(97.9109), tensor(94.3280), tensor(95.6579), tensor(95.4917), tensor(94.5361), tensor(94.6048), tensor(97.6204), tensor(94.2664), tensor(95.1992), tensor(95.5744), tensor(95.6779), tensor(94.7328), tensor(95.2107), tensor(94.9279), tensor(94.4513), tensor(95.0953), tensor(95.6788), tensor(95.1394), tensor(94.5240), tensor(98.0069), tensor(94.9517), tensor(95.5284), tensor(95.2128), tensor(95.0571), tensor(95.0739), tensor(95.1861), tensor(95.2079), tensor(95.0758), tensor(94.2360), tensor(95.2226), tensor(94.8801), tensor(94.0295), tensor(94.1590), tensor(95.0628), tensor(95.5981), tensor(95.0211), tensor(94.6106), tensor(95.0514), tensor(95.4503), tensor(94.7613), tensor(95.4885), tensor(98.0799), tensor(94.2750), tensor(95.0498), tensor(95.4025), tensor(94.7707), tensor(95.7168), tensor(94.3912), tensor(95.5329), tensor(94.7679), tensor(94.7703), tensor(95.4993), tensor(95.6933), tensor(95.4274), tensor(95.5593), tensor(95.3304), tensor(95.0030), tensor(94.7260), tensor(94.5150), tensor(95.1419), tensor(94.8718), tensor(96.2530), tensor(94.5481), tensor(94.7976), tensor(94.1302), tensor(95.4155), tensor(94.6513), tensor(95.1610), tensor(95.2127), tensor(95.1342)]\n"
     ]
    }
   ],
   "source": [
    "# an alternative approach is to use text embeddings instead of the information theoretic features\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers.util import cos_sim\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "model_path = 'thenlper/gte-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(\"cuda:2\")\n",
    "batch_dict = tokenizer(df[\"creative_scenario_round_4\"].to_list(), max_length=8192, padding=True, truncation=True, return_tensors='pt').to(\"cuda:2\")\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = outputs.last_hidden_state[:, 0]\n",
    " \n",
    "# (Optionally) normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "scores = (embeddings @ embeddings.T) * 100\n",
    "scores = list(scores[0].detach().cpu())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [float(x) for x in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_similarity\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creative_scenario_round_4</th>\n",
       "      <th>complexity_aggregrate</th>\n",
       "      <th>difficulty_aggregrate</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>fano_factor</th>\n",
       "      <th>ARI</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noah is a high school student who loves to gar...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.365829</td>\n",
       "      <td>7.946099</td>\n",
       "      <td>3.215488</td>\n",
       "      <td>6.294411</td>\n",
       "      <td>0.562842</td>\n",
       "      <td>99.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy is a barista at Lily's Cafe. She has been ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.763808</td>\n",
       "      <td>7.803133</td>\n",
       "      <td>3.149425</td>\n",
       "      <td>6.182933</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>95.372375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy works at Lily's Cafe. She has been working...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.499628</td>\n",
       "      <td>5.493406</td>\n",
       "      <td>1.913696</td>\n",
       "      <td>5.235199</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>95.359215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucas has been working at the local bookstore ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.466565</td>\n",
       "      <td>4.887156</td>\n",
       "      <td>1.451532</td>\n",
       "      <td>6.491390</td>\n",
       "      <td>0.567742</td>\n",
       "      <td>95.004692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hannah was working at the library when she not...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.651710</td>\n",
       "      <td>3.574473</td>\n",
       "      <td>0.817125</td>\n",
       "      <td>6.201779</td>\n",
       "      <td>0.575540</td>\n",
       "      <td>94.964203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           creative_scenario_round_4  complexity_aggregrate  \\\n",
       "0  Noah is a high school student who loves to gar...                      3   \n",
       "1  Amy is a barista at Lily's Cafe. She has been ...                      2   \n",
       "2  Amy works at Lily's Cafe. She has been working...                      2   \n",
       "3  Lucas has been working at the local bookstore ...                      2   \n",
       "4  Hannah was working at the library when she not...                      3   \n",
       "\n",
       "   difficulty_aggregrate  perplexity  burstiness  fano_factor       ARI  \\\n",
       "0                      2    3.365829    7.946099     3.215488  6.294411   \n",
       "1                      2    3.763808    7.803133     3.149425  6.182933   \n",
       "2                      3    3.499628    5.493406     1.913696  5.235199   \n",
       "3                      1    3.466565    4.887156     1.451532  6.491390   \n",
       "4                      2    5.651710    3.574473     0.817125  6.201779   \n",
       "\n",
       "        ttr  mean_similarity  \n",
       "0  0.562842        99.999992  \n",
       "1  0.580000        95.372375  \n",
       "2  0.573034        95.359215  \n",
       "3  0.567742        95.004692  \n",
       "4  0.575540        94.964203  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creative_scenario_round_4</th>\n",
       "      <th>complexity_aggregrate</th>\n",
       "      <th>difficulty_aggregrate</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>fano_factor</th>\n",
       "      <th>ARI</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noah is a high school student who loves to gar...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.781108</td>\n",
       "      <td>1.061222</td>\n",
       "      <td>1.131066</td>\n",
       "      <td>-0.943893</td>\n",
       "      <td>-0.112595</td>\n",
       "      <td>6.496502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy is a barista at Lily's Cafe. She has been ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.565861</td>\n",
       "      <td>0.974678</td>\n",
       "      <td>1.068521</td>\n",
       "      <td>-1.004129</td>\n",
       "      <td>0.175056</td>\n",
       "      <td>0.343908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy works at Lily's Cafe. She has been working...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.708742</td>\n",
       "      <td>-0.423512</td>\n",
       "      <td>-0.101421</td>\n",
       "      <td>-1.516228</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.326411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           creative_scenario_round_4  complexity_aggregrate  \\\n",
       "0  Noah is a high school student who loves to gar...                      3   \n",
       "1  Amy is a barista at Lily's Cafe. She has been ...                      2   \n",
       "2  Amy works at Lily's Cafe. She has been working...                      2   \n",
       "\n",
       "   difficulty_aggregrate  perplexity  burstiness  fano_factor       ARI  \\\n",
       "0                      2   -0.781108    1.061222     1.131066 -0.943893   \n",
       "1                      2   -0.565861    0.974678     1.068521 -1.004129   \n",
       "2                      3   -0.708742   -0.423512    -0.101421 -1.516228   \n",
       "\n",
       "        ttr  mean_similarity  \n",
       "0 -0.112595         6.496502  \n",
       "1  0.175056         0.343908  \n",
       "2  0.058270         0.326411  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply standard scaler to all features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[\n",
    "    [\"perplexity\", \"burstiness\", \"ARI\", \"ttr\", \"fano_factor\", \"mean_similarity\"]\n",
    "] = scaler.fit_transform(\n",
    "    df[\n",
    "        [\n",
    "            \"perplexity\",\n",
    "            \"burstiness\",\n",
    "            \"ARI\",\n",
    "            \"ttr\",\n",
    "            \"fano_factor\",\n",
    "            \"mean_similarity\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pre-computed features\n",
    "# df.to_csv(\"/home/aml7990/Code/creativity-item-generation/item_evaluation/linear_regression/llama-3-8b-instruct-full-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfidenceInterval(low=0.1968258677175594, high=0.20568798167434388)\n",
      "0.20164367938065092\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "from statistics import median\n",
    "\n",
    "n_samples = 1000\n",
    "frac = 0.9\n",
    "results = []\n",
    "# we use a boostrapping approach to fit a CI on the adjusted r squared\n",
    "\n",
    "for sample in range(n_samples):\n",
    "    sample = df.sample(frac=frac, replace=True)\n",
    "    Y = sample[\"complexity_aggregrate\"].to_numpy()\n",
    "    X = sample[[\"perplexity\", \"burstiness\", \"ARI\", \"ttr\", \"fano_factor\", \"mean_similarity\"]].to_numpy()\n",
    "    # X = sample[[\"perplexity\", \"readability\", \"ttr\", \"fano_factor\"]].to_numpy()\n",
    "    \n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    results.append(model.rsquared_adj)\n",
    "\n",
    "# calculate 95% bootstrapped confidence interval for median\n",
    "bootstrap_ci = bootstrap((results,), np.median, confidence_level=0.95, random_state=1, method='percentile')\n",
    "\n",
    "print(bootstrap_ci.confidence_interval)\n",
    "print(median(results))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [[\"perplexity\", \"readability\", \"ttr\", \"fano_factor\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10725822571215704,\n",
       " 0.1912212331814499,\n",
       " 0.28301260164928355,\n",
       " 0.2721416611099976,\n",
       " 0.17569914068600057,\n",
       " 0.27006827121233046,\n",
       " 0.19179706201851898,\n",
       " 0.17488301675236417,\n",
       " 0.15078144671466343,\n",
       " 0.1609815007722727,\n",
       " 0.2587854275047806,\n",
       " 0.16174562309882456,\n",
       " 0.20244327692221042,\n",
       " 0.19286394438249377,\n",
       " 0.24339396410465197,\n",
       " 0.24490271198687164,\n",
       " 0.214330891612182,\n",
       " 0.14374607748954893,\n",
       " 0.21486755696440563,\n",
       " 0.20205993375277786,\n",
       " 0.15919758728535593,\n",
       " 0.12507550682038737,\n",
       " 0.16689922749508257,\n",
       " 0.254780198332734,\n",
       " 0.19427611634991637,\n",
       " 0.14561229025965505,\n",
       " 0.175720335802516,\n",
       " 0.16050196333695332,\n",
       " 0.12434041014179098,\n",
       " 0.20997157179807402,\n",
       " 0.18021860093997322,\n",
       " 0.2746495430496577,\n",
       " 0.18466307291622008,\n",
       " 0.16670127286940295,\n",
       " 0.2794849614286683,\n",
       " 0.2687881626390407,\n",
       " 0.16712147666743893,\n",
       " 0.21740844838981832,\n",
       " 0.15750504428596446,\n",
       " 0.26120406828473663,\n",
       " 0.12907781523661277,\n",
       " 0.14801614061223245,\n",
       " 0.2758761315902909,\n",
       " 0.28354235337980926,\n",
       " 0.2478619763815303,\n",
       " 0.1891395307020718,\n",
       " 0.23018212814278183,\n",
       " 0.25980618164188807,\n",
       " 0.14944202251871952,\n",
       " 0.24260073626202805,\n",
       " 0.15313091444520877,\n",
       " 0.23888346402660077,\n",
       " 0.3302006966760209,\n",
       " 0.25169852216031185,\n",
       " 0.1642563570254323,\n",
       " 0.1648647334331773,\n",
       " 0.13839423143972795,\n",
       " 0.24509291843553915,\n",
       " 0.22300589908959123,\n",
       " 0.13730448438326404,\n",
       " 0.2267821450803127,\n",
       " 0.2095790221210524,\n",
       " 0.19140027871613474,\n",
       " 0.23368467288364847,\n",
       " 0.21636059944508046,\n",
       " 0.16650064853354174,\n",
       " 0.24383492743792934,\n",
       " 0.22456585865723977,\n",
       " 0.1698921870933252,\n",
       " 0.17368390758706742,\n",
       " 0.2332771090503033,\n",
       " 0.20295753418704643,\n",
       " 0.1086318200250499,\n",
       " 0.22219552849095214,\n",
       " 0.17043882295759594,\n",
       " 0.1284912596236144,\n",
       " 0.29491562295031337,\n",
       " 0.20416767653123447,\n",
       " 0.2910960745832798,\n",
       " 0.21054153266192133,\n",
       " 0.24916931428033418,\n",
       " 0.1691357685182654,\n",
       " 0.20170766443620713,\n",
       " 0.24667120665147413,\n",
       " 0.2591975015878758,\n",
       " 0.20031559124682685,\n",
       " 0.24073235235479118,\n",
       " 0.2226885902390171,\n",
       " 0.14655470518620572,\n",
       " 0.20489387237109535,\n",
       " 0.19731133555618885,\n",
       " 0.24219284030209476,\n",
       " 0.24871540823754323,\n",
       " 0.16323883627277236,\n",
       " 0.14491456688206317,\n",
       " 0.17739007071579393,\n",
       " 0.22028829415809714,\n",
       " 0.22079714685959406,\n",
       " 0.19029506635775284,\n",
       " 0.2448167438260168,\n",
       " 0.24657416292233358,\n",
       " 0.1986492253719654,\n",
       " 0.13815093899322106,\n",
       " 0.26904700619311206,\n",
       " 0.20399328134006023,\n",
       " 0.17253359011084957,\n",
       " 0.1573612714725755,\n",
       " 0.1786436682648268,\n",
       " 0.1806182061384688,\n",
       " 0.260183425708842,\n",
       " 0.1380785805497461,\n",
       " 0.14582400739958967,\n",
       " 0.21381362085366562,\n",
       " 0.17586154045197444,\n",
       " 0.1518749413482724,\n",
       " 0.23929420878655394,\n",
       " 0.1292569263341531,\n",
       " 0.19169079753796114,\n",
       " 0.12927077278471977,\n",
       " 0.13941972820380144,\n",
       " 0.2401426696600214,\n",
       " 0.25682321167631106,\n",
       " 0.186594943841626,\n",
       " 0.19958293901069357,\n",
       " 0.17364792842745191,\n",
       " 0.28365904240864037,\n",
       " 0.26883667118222865,\n",
       " 0.2263616537247254,\n",
       " 0.1652463585920867,\n",
       " 0.1783248477976136,\n",
       " 0.15355653266174962,\n",
       " 0.3176329476394457,\n",
       " 0.15571350127314787,\n",
       " 0.15498228538061432,\n",
       " 0.20849683754926962,\n",
       " 0.2155371608242309,\n",
       " 0.2154581011025528,\n",
       " 0.1859771762395075,\n",
       " 0.17595769139796913,\n",
       " 0.19184012458674948,\n",
       " 0.17970482404500188,\n",
       " 0.1704688881385208,\n",
       " 0.24570915943230331,\n",
       " 0.11106831329201095,\n",
       " 0.229086118419067,\n",
       " 0.2274763689106255,\n",
       " 0.17954752442865518,\n",
       " 0.23571894345461464,\n",
       " 0.2698900524992053,\n",
       " 0.16682067612296125,\n",
       " 0.21301219356996126,\n",
       " 0.25479782778194715,\n",
       " 0.23185559401063238,\n",
       " 0.2500161781821666,\n",
       " 0.23925436912781028,\n",
       " 0.22563554371001815,\n",
       " 0.2047587845659803,\n",
       " 0.20687136337425605,\n",
       " 0.2488322939223545,\n",
       " 0.1823051784882218,\n",
       " 0.2977063368306777,\n",
       " 0.23963603606041217,\n",
       " 0.2902350006595992,\n",
       " 0.2170421817758409,\n",
       " 0.21566621845581602,\n",
       " 0.2586258876644907,\n",
       " 0.15812551082096737,\n",
       " 0.17845633192539945,\n",
       " 0.3308750051873569,\n",
       " 0.2754401360307789,\n",
       " 0.250064337718921,\n",
       " 0.19800493567726485,\n",
       " 0.15713999389001587,\n",
       " 0.2690079671725377,\n",
       " 0.20549690406105592,\n",
       " 0.20679563923490096,\n",
       " 0.23123523101344512,\n",
       " 0.20544401225987785,\n",
       " 0.1951523647676292,\n",
       " 0.19760541363836248,\n",
       " 0.28596681739341157,\n",
       " 0.3058574944538478,\n",
       " 0.19845290650779457,\n",
       " 0.2684273742026331,\n",
       " 0.26205218695203014,\n",
       " 0.1840700538727068,\n",
       " 0.2282074465961168,\n",
       " 0.3023333063593677,\n",
       " 0.243057012274301,\n",
       " 0.16017137909015045,\n",
       " 0.25378645984667736,\n",
       " 0.27415390280602847,\n",
       " 0.27384628390645527,\n",
       " 0.22199076608625967,\n",
       " 0.1274841599197447,\n",
       " 0.13557989992549424,\n",
       " 0.24322381501996437,\n",
       " 0.15139851639093227,\n",
       " 0.12847822619487714,\n",
       " 0.0777910983726795,\n",
       " 0.22483211268367675,\n",
       " 0.11363673966067422,\n",
       " 0.20342656180375596,\n",
       " 0.14777462201658065,\n",
       " 0.22852253575218073,\n",
       " 0.18650188885892272,\n",
       " 0.12858040929918224,\n",
       " 0.17426637270431622,\n",
       " 0.1864679908248199,\n",
       " 0.16288143216909667,\n",
       " 0.23985168944251734,\n",
       " 0.09064493149002018,\n",
       " 0.10313844005407802,\n",
       " 0.1401045333099541,\n",
       " 0.1345076492281122,\n",
       " 0.24541654824143022,\n",
       " 0.14801408482546363,\n",
       " 0.2805901533692715,\n",
       " 0.14425230404180367,\n",
       " 0.24817215048342123,\n",
       " 0.24873728482557633,\n",
       " 0.1969090961259743,\n",
       " 0.21303006642031042,\n",
       " 0.1888738428858031,\n",
       " 0.15014602097773933,\n",
       " 0.2119008248070391,\n",
       " 0.2181640343321566,\n",
       " 0.15896994440559664,\n",
       " 0.2676200549035539,\n",
       " 0.3267833838581484,\n",
       " 0.18525644705830802,\n",
       " 0.11227273467542787,\n",
       " 0.15296100650912936,\n",
       " 0.21842324004941482,\n",
       " 0.13167916674116387,\n",
       " 0.35318017022967907,\n",
       " 0.36107379032936915,\n",
       " 0.20124327837915057,\n",
       " 0.15717370674426057,\n",
       " 0.16627742318945016,\n",
       " 0.20840887662092134,\n",
       " 0.13144064569043823,\n",
       " 0.24301015777039092,\n",
       " 0.2018186260771343,\n",
       " 0.2082167474054809,\n",
       " 0.1707271874990739,\n",
       " 0.2008054471788766,\n",
       " 0.17801515111949384,\n",
       " 0.2295442973920836,\n",
       " 0.20798493736771118,\n",
       " 0.21735275143799015,\n",
       " 0.163746996674354,\n",
       " 0.19883294055460743,\n",
       " 0.14724452442691982,\n",
       " 0.1752519799630684,\n",
       " 0.2639910340116087,\n",
       " 0.24338647042722072,\n",
       " 0.22290303429706126,\n",
       " 0.2296572090818112,\n",
       " 0.18129042160611453,\n",
       " 0.10871880452327587,\n",
       " 0.22592444229406838,\n",
       " 0.18246876808190038,\n",
       " 0.2525710356548806,\n",
       " 0.18934993808209322,\n",
       " 0.13202237760267965,\n",
       " 0.2492372302786786,\n",
       " 0.23732306539939496,\n",
       " 0.11521082577409214,\n",
       " 0.2788826076227937,\n",
       " 0.21834147881007904,\n",
       " 0.2384526043947479,\n",
       " 0.18784491974968343,\n",
       " 0.1882802425819773,\n",
       " 0.17935848864748172,\n",
       " 0.14914550552367956,\n",
       " 0.18154888514153744,\n",
       " 0.18784702044666823,\n",
       " 0.23518164392287222,\n",
       " 0.18734440237006178,\n",
       " 0.15589715447041308,\n",
       " 0.1927660519386729,\n",
       " 0.24753322670909106,\n",
       " 0.18697445893922848,\n",
       " 0.1423988882661109,\n",
       " 0.2938567548666865,\n",
       " 0.2591280885985685,\n",
       " 0.24437398811357236,\n",
       " 0.2150151394154386,\n",
       " 0.13320889804872216,\n",
       " 0.24993841583896215,\n",
       " 0.19143570291799128,\n",
       " 0.14849285097711018,\n",
       " 0.18686314300274576,\n",
       " 0.2815810592381732,\n",
       " 0.17465641973516932,\n",
       " 0.2655960430507759,\n",
       " 0.17935288517731318,\n",
       " 0.33645920506755134,\n",
       " 0.1444272985778371,\n",
       " 0.14477413068711964,\n",
       " 0.18435976114744035,\n",
       " 0.24640632390301875,\n",
       " 0.2537073204593715,\n",
       " 0.19237188711705222,\n",
       " 0.23549121378587823,\n",
       " 0.17652960786882232,\n",
       " 0.24241036405276073,\n",
       " 0.1697449972923888,\n",
       " 0.15291537952008338,\n",
       " 0.22220055145554385,\n",
       " 0.22344153884755735,\n",
       " 0.28685884523399585,\n",
       " 0.22102332290715976,\n",
       " 0.17476425472445956,\n",
       " 0.2112037970530508,\n",
       " 0.20457505912070406,\n",
       " 0.1637764670385793,\n",
       " 0.14324959512580515,\n",
       " 0.24516580123163945,\n",
       " 0.17759409797377768,\n",
       " 0.13804491334316604,\n",
       " 0.2020616549938571,\n",
       " 0.2183167751168782,\n",
       " 0.2500894994312052,\n",
       " 0.17710575665613348,\n",
       " 0.11314491493372247,\n",
       " 0.24424120400991545,\n",
       " 0.17811844699510604,\n",
       " 0.2961391460559052,\n",
       " 0.1733540537445114,\n",
       " 0.16343644773431887,\n",
       " 0.17329345213176472,\n",
       " 0.22791930115742742,\n",
       " 0.2258255973440204,\n",
       " 0.22121402751698727,\n",
       " 0.20616816071795663,\n",
       " 0.20742610193491917,\n",
       " 0.1721123617102398,\n",
       " 0.1609925995183985,\n",
       " 0.19815089485671233,\n",
       " 0.18060664252801717,\n",
       " 0.21712266340897912,\n",
       " 0.29044844650523294,\n",
       " 0.14548553799727226,\n",
       " 0.17689141489402116,\n",
       " 0.17869190255413658,\n",
       " 0.1563702237994231,\n",
       " 0.28193845473145995,\n",
       " 0.22573067569050642,\n",
       " 0.19324312627273288,\n",
       " 0.17279636479779314,\n",
       " 0.21963522508530597,\n",
       " 0.24317180843026343,\n",
       " 0.22574839101817779,\n",
       " 0.24187712958337027,\n",
       " 0.17821464609451254,\n",
       " 0.2744669454039115,\n",
       " 0.11952705735985913,\n",
       " 0.23413286525338606,\n",
       " 0.20601268755996094,\n",
       " 0.22690687378793084,\n",
       " 0.1399534663132539,\n",
       " 0.23583666026978511,\n",
       " 0.217449867388526,\n",
       " 0.1261449315009917,\n",
       " 0.1947898698209719,\n",
       " 0.17496821873226398,\n",
       " 0.28246434395732034,\n",
       " 0.09796869445571921,\n",
       " 0.16614233651473143,\n",
       " 0.2818750854557214,\n",
       " 0.22160864643045375,\n",
       " 0.22068731736820235,\n",
       " 0.18393974971139748,\n",
       " 0.27868676694667815,\n",
       " 0.16909968551158683,\n",
       " 0.22636828625716443,\n",
       " 0.16079079569529808,\n",
       " 0.2040807971537748,\n",
       " 0.24835668316246717,\n",
       " 0.21069071887790602,\n",
       " 0.22368241571332093,\n",
       " 0.20319903938883155,\n",
       " 0.24206889599682546,\n",
       " 0.1809448429877949,\n",
       " 0.21532570834773024,\n",
       " 0.28128139562605237,\n",
       " 0.18083021202423766,\n",
       " 0.2234243766447972,\n",
       " 0.22411704143949185,\n",
       " 0.17442063470769498,\n",
       " 0.07172303570123117,\n",
       " 0.11773246916338331,\n",
       " 0.22497111314536467,\n",
       " 0.1617084469217498,\n",
       " 0.16333976489206437,\n",
       " 0.13559164256984635,\n",
       " 0.2802913934314505,\n",
       " 0.129448436068754,\n",
       " 0.22445113722401488,\n",
       " 0.15816471240790542,\n",
       " 0.325971564845113,\n",
       " 0.2273492235789475,\n",
       " 0.16390222011004496,\n",
       " 0.22495012590490826,\n",
       " 0.16202243326258559,\n",
       " 0.22215427168492707,\n",
       " 0.29807508602163724,\n",
       " 0.17603541373872977,\n",
       " 0.20402831242646335,\n",
       " 0.20697212821972266,\n",
       " 0.11962884157521425,\n",
       " 0.22284925511601872,\n",
       " 0.1607990722669449,\n",
       " 0.15392514401697588,\n",
       " 0.22330994668754622,\n",
       " 0.22348990867750407,\n",
       " 0.23046321082026167,\n",
       " 0.1651646614606097,\n",
       " 0.2673724576121608,\n",
       " 0.13754882179189576,\n",
       " 0.26068293222117533,\n",
       " 0.31336542267047096,\n",
       " 0.1597463362078424,\n",
       " 0.26155200971568315,\n",
       " 0.25758903028815905,\n",
       " 0.16860408604400468,\n",
       " 0.11809971674947861,\n",
       " 0.19987605029976396,\n",
       " 0.24577883857974547,\n",
       " 0.2158816890209131,\n",
       " 0.22298710013642187,\n",
       " 0.24200914054170197,\n",
       " 0.25801569649569933,\n",
       " 0.14162277107272148,\n",
       " 0.14557631992042952,\n",
       " 0.15396489518826095,\n",
       " 0.10154884801883479,\n",
       " 0.22582036600341515,\n",
       " 0.22982746913063856,\n",
       " 0.12929367605893216,\n",
       " 0.20808963960226745,\n",
       " 0.21110698176482046,\n",
       " 0.1697660768779784,\n",
       " 0.24048587719107706,\n",
       " 0.13439015309622204,\n",
       " 0.2428134902320378,\n",
       " 0.10845752812389109,\n",
       " 0.14648650811995967,\n",
       " 0.28275138624092255,\n",
       " 0.17550766223205738,\n",
       " 0.23066923935598116,\n",
       " 0.11983725805698908,\n",
       " 0.1499992305177551,\n",
       " 0.16920224637750902,\n",
       " 0.21102165786438565,\n",
       " 0.16343975462139693,\n",
       " 0.15538649881974786,\n",
       " 0.17246845626891938,\n",
       " 0.12993190700475277,\n",
       " 0.18085323788331964,\n",
       " 0.2405412843653394,\n",
       " 0.15643283394838503,\n",
       " 0.16382996205634315,\n",
       " 0.18367481640208616,\n",
       " 0.1836749710501503,\n",
       " 0.2832423136943898,\n",
       " 0.2020259370370613,\n",
       " 0.2612416757073154,\n",
       " 0.19114800053904935,\n",
       " 0.13977602022024294,\n",
       " 0.2612410444631429,\n",
       " 0.2054815204773226,\n",
       " 0.22498591197993345,\n",
       " 0.19764016574869114,\n",
       " 0.15554974879696115,\n",
       " 0.18753582756981035,\n",
       " 0.24057368538003543,\n",
       " 0.1960919637683528,\n",
       " 0.18404532729814216,\n",
       " 0.1462141435568287,\n",
       " 0.291209713991758,\n",
       " 0.1944411473045109,\n",
       " 0.2606104298585902,\n",
       " 0.24243694723132314,\n",
       " 0.21572858245419702,\n",
       " 0.23321670307646114,\n",
       " 0.20229135464046588,\n",
       " 0.2800379560348788,\n",
       " 0.21484364803476974,\n",
       " 0.16569340172788882,\n",
       " 0.22459823119420985,\n",
       " 0.2240366008714243,\n",
       " 0.2430606493781382,\n",
       " 0.17394533847809812,\n",
       " 0.21141957050432314,\n",
       " 0.22339737254233805,\n",
       " 0.2114230448361386,\n",
       " 0.348659832508869,\n",
       " 0.13117125464129464,\n",
       " 0.18857069819297723,\n",
       " 0.22083639083326034,\n",
       " 0.2617428900448068,\n",
       " 0.28689073640589013,\n",
       " 0.2155862963968539,\n",
       " 0.20481752222015293,\n",
       " 0.11373089783625823,\n",
       " 0.2740716274032914,\n",
       " 0.17736660124659054,\n",
       " 0.18554765190735,\n",
       " 0.1952307836566416,\n",
       " 0.1913686528379529,\n",
       " 0.18741012938883472,\n",
       " 0.20249316203209644,\n",
       " 0.16004148905028748,\n",
       " 0.2751282050666316,\n",
       " 0.20856314045920887,\n",
       " 0.24232969135034532,\n",
       " 0.19204845916123758,\n",
       " 0.15971540093239833,\n",
       " 0.13325991521101177,\n",
       " 0.12594688117251285,\n",
       " 0.19750027751485522,\n",
       " 0.19379853592584206,\n",
       " 0.18887953518634681,\n",
       " 0.18303428050714932,\n",
       " 0.20175337410694238,\n",
       " 0.26530234026937893,\n",
       " 0.2283073608283479,\n",
       " 0.15058657632874162,\n",
       " 0.23960325881280076,\n",
       " 0.1801484102900872,\n",
       " 0.20725639513761107,\n",
       " 0.1876043433809027,\n",
       " 0.1709259438517282,\n",
       " 0.30927707903587465,\n",
       " 0.1498545835618761,\n",
       " 0.16047591246972648,\n",
       " 0.1940361492184436,\n",
       " 0.25992825127698693,\n",
       " 0.25813131904069697,\n",
       " 0.2497542498093822,\n",
       " 0.16649339869432245,\n",
       " 0.23457927137486467,\n",
       " 0.20101613184434552,\n",
       " 0.2466076865392519,\n",
       " 0.11942214927884265,\n",
       " 0.1962048907104772,\n",
       " 0.24817174332200487,\n",
       " 0.1088711351025784,\n",
       " 0.23997174147748346,\n",
       " 0.12851766692514266,\n",
       " 0.13045738087965253,\n",
       " 0.24244788768778847,\n",
       " 0.15805998584363923,\n",
       " 0.25506552832899154,\n",
       " 0.14304543809502646,\n",
       " 0.12881006659579652,\n",
       " 0.16222274277213888,\n",
       " 0.2799542960462176,\n",
       " 0.17123129820425842,\n",
       " 0.26278105961621445,\n",
       " 0.218129133744707,\n",
       " 0.16402297461410842,\n",
       " 0.15482986089728945,\n",
       " 0.3228836695737356,\n",
       " 0.10233643036862416,\n",
       " 0.1709483221544792,\n",
       " 0.2386588350291794,\n",
       " 0.1856203067416713,\n",
       " 0.16895407573044086,\n",
       " 0.28412768765464047,\n",
       " 0.2028230695684221,\n",
       " 0.16314067161353218,\n",
       " 0.21580027110027433,\n",
       " 0.19415630218404856,\n",
       " 0.2701442634206588,\n",
       " 0.2354680600928799,\n",
       " 0.24191994264184047,\n",
       " 0.13676648387163437,\n",
       " 0.12002506922784428,\n",
       " 0.16251267108082101,\n",
       " 0.3329050914772951,\n",
       " 0.21069815865427255,\n",
       " 0.22226911666350524,\n",
       " 0.2721535215673211,\n",
       " 0.14853330582352864,\n",
       " 0.23803996080306355,\n",
       " 0.18921083041584863,\n",
       " 0.2901659938674094,\n",
       " 0.24125314442250057,\n",
       " 0.17965661071396466,\n",
       " 0.19955006922055707,\n",
       " 0.26347756381511833,\n",
       " 0.11784323879411018,\n",
       " 0.1840510152420456,\n",
       " 0.2030336067107622,\n",
       " 0.22687237349621436,\n",
       " 0.1935439393503533,\n",
       " 0.14063296237766132,\n",
       " 0.24491357301402206,\n",
       " 0.216749096583858,\n",
       " 0.2003145553226806,\n",
       " 0.268854418103193,\n",
       " 0.1487934816985309,\n",
       " 0.23912378623326203,\n",
       " 0.1261223057328198,\n",
       " 0.2896620571907442,\n",
       " 0.19586341610108282,\n",
       " 0.18673543671488202,\n",
       " 0.2428891308866351,\n",
       " 0.18133052149569862,\n",
       " 0.1953916056037861,\n",
       " 0.17721148792165564,\n",
       " 0.18159419011597688,\n",
       " 0.15601587377196802,\n",
       " 0.18205099274077008,\n",
       " 0.20048299439965978,\n",
       " 0.2297882205481876,\n",
       " 0.163160311799864,\n",
       " 0.2929502625881175,\n",
       " 0.11909283090298739,\n",
       " 0.17711474919671932,\n",
       " 0.27294557468032565,\n",
       " 0.30219286242147814,\n",
       " 0.24089444327777143,\n",
       " 0.19006705282932135,\n",
       " 0.15706489484343888,\n",
       " 0.1939932635169075,\n",
       " 0.2819780892847932,\n",
       " 0.17774897051008454,\n",
       " 0.22701441394383526,\n",
       " 0.1050102071323149,\n",
       " 0.17288113263358862,\n",
       " 0.20567089327947508,\n",
       " 0.20660255862040144,\n",
       " 0.15752115531263666,\n",
       " 0.12667561807331007,\n",
       " 0.26099047710534595,\n",
       " 0.25682410868570826,\n",
       " 0.29680163498391665,\n",
       " 0.20892626553279603,\n",
       " 0.2143527442410097,\n",
       " 0.20188099831528517,\n",
       " 0.27476737034944365,\n",
       " 0.16277241798561926,\n",
       " 0.14338299885080574,\n",
       " 0.22386975351077631,\n",
       " 0.20285462025100742,\n",
       " 0.17279935973484273,\n",
       " 0.14334106197338703,\n",
       " 0.23267390003816435,\n",
       " 0.2268471088934061,\n",
       " 0.24723925738427588,\n",
       " 0.12029671107430429,\n",
       " 0.18498313354378793,\n",
       " 0.19293021721266812,\n",
       " 0.2233263771098538,\n",
       " 0.2971608219592088,\n",
       " 0.16444727950874005,\n",
       " 0.20605531082677353,\n",
       " 0.20597130479351766,\n",
       " 0.18518046442689062,\n",
       " 0.1568437557075676,\n",
       " 0.22624622971164365,\n",
       " 0.24911147340038164,\n",
       " 0.20478737923648838,\n",
       " 0.12860080255413187,\n",
       " 0.22733301873429312,\n",
       " 0.3080251364237918,\n",
       " 0.16347417150226884,\n",
       " 0.0978270001095094,\n",
       " 0.20133503082826631,\n",
       " 0.20246009541662546,\n",
       " 0.22241813268488098,\n",
       " 0.2979353035895276,\n",
       " 0.14108462002340516,\n",
       " 0.20693485473595818,\n",
       " 0.12033522687690223,\n",
       " 0.2172422936270303,\n",
       " 0.24429953608606925,\n",
       " 0.22278254492705696,\n",
       " 0.14532906216123997,\n",
       " 0.14546988494110058,\n",
       " 0.14602725414752704,\n",
       " 0.19503994455416584,\n",
       " 0.2314780507288977,\n",
       " 0.16541587754818998,\n",
       " 0.16952344603842784,\n",
       " 0.23719257939876826,\n",
       " 0.1838858536207143,\n",
       " 0.21742031710875787,\n",
       " 0.22161745897087304,\n",
       " 0.19297349196896219,\n",
       " 0.16367180203105058,\n",
       " 0.14965749781733528,\n",
       " 0.17146451488976067,\n",
       " 0.28279318405713905,\n",
       " 0.2599679457705004,\n",
       " 0.2070710643391146,\n",
       " 0.14039727423358994,\n",
       " 0.12230816248554122,\n",
       " 0.2370956010442381,\n",
       " 0.2062199606822297,\n",
       " 0.20303851603985634,\n",
       " 0.23181568378530226,\n",
       " 0.26130335711314767,\n",
       " 0.19660184149335957,\n",
       " 0.1518957085886472,\n",
       " 0.2507329790979944,\n",
       " 0.2006245076463038,\n",
       " 0.19508426676644786,\n",
       " 0.2580954091264923,\n",
       " 0.23575627515818787,\n",
       " 0.1669075490789026,\n",
       " 0.2188625529496412,\n",
       " 0.22314538648256788,\n",
       " 0.2619364304876711,\n",
       " 0.21205035619725987,\n",
       " 0.1332299260930474,\n",
       " 0.24864800981020407,\n",
       " 0.16686808314064228,\n",
       " 0.20157701510031567,\n",
       " 0.18590152886368982,\n",
       " 0.159411187072365,\n",
       " 0.25419645528308654,\n",
       " 0.17411556718798038,\n",
       " 0.1800092406275453,\n",
       " 0.18972613601917432,\n",
       " 0.14989981984585554,\n",
       " 0.20267241574809025,\n",
       " 0.2865534937696128,\n",
       " 0.1349308705854676,\n",
       " 0.21397845723241493,\n",
       " 0.19919994568970467,\n",
       " 0.2025020029271073,\n",
       " 0.15915154748335525,\n",
       " 0.2552242848260582,\n",
       " 0.22149974525663163,\n",
       " 0.18791392348926828,\n",
       " 0.19835521769924658,\n",
       " 0.27175184647003026,\n",
       " 0.20216297343241507,\n",
       " 0.12712535055878982,\n",
       " 0.12101073714784039,\n",
       " 0.15052860029362614,\n",
       " 0.2257363448677263,\n",
       " 0.24453801603168512,\n",
       " 0.235529529251836,\n",
       " 0.22929827492420407,\n",
       " 0.14311614411200324,\n",
       " 0.15745874584392394,\n",
       " 0.16856169897190876,\n",
       " 0.20716705543111158,\n",
       " 0.25105146725471683,\n",
       " 0.1839542960378433,\n",
       " 0.13698274389583587,\n",
       " 0.20061026109443703,\n",
       " 0.1836453908624759,\n",
       " 0.11922763898024602,\n",
       " 0.2475721945952858,\n",
       " 0.2029778120898088,\n",
       " 0.2067485701701478,\n",
       " 0.23939586115948208,\n",
       " 0.23920185234915503,\n",
       " 0.15119147952527867,\n",
       " 0.13301756261634246,\n",
       " 0.18786553867545475,\n",
       " 0.2038948100071738,\n",
       " 0.2168614691275189,\n",
       " 0.14873489900704406,\n",
       " 0.24122178566160668,\n",
       " 0.2277544554005716,\n",
       " 0.21979525474939876,\n",
       " 0.18393916604656513,\n",
       " 0.1663225081956834,\n",
       " 0.11756718130661858,\n",
       " 0.16060593450383853,\n",
       " 0.15457299991959705,\n",
       " 0.21163556517075577,\n",
       " 0.13853328128768228,\n",
       " 0.15330861792733008,\n",
       " 0.15525537770158415,\n",
       " 0.20629049054204085,\n",
       " 0.2461865260985403,\n",
       " 0.23917929694478424,\n",
       " 0.26437510538409603,\n",
       " 0.15750599879069194,\n",
       " 0.23489096076755955,\n",
       " 0.23955060307781373,\n",
       " 0.20001709050469252,\n",
       " 0.2204970608112874,\n",
       " 0.17365726745107823,\n",
       " 0.2863135999813615,\n",
       " 0.22276111660622722,\n",
       " 0.16095428975255544,\n",
       " 0.10958967813718357,\n",
       " 0.13140834309596572,\n",
       " 0.29224661390937945,\n",
       " 0.21386204022782207,\n",
       " 0.07657753246839494,\n",
       " 0.21042569790263932,\n",
       " 0.1863430178689197,\n",
       " 0.23004260066681248,\n",
       " 0.16262316920754982,\n",
       " 0.20014600845732478,\n",
       " 0.1802524245323579,\n",
       " 0.15527027621515643,\n",
       " 0.2619403840832174,\n",
       " 0.21773255058284657,\n",
       " 0.2523657749999816,\n",
       " 0.19131231397349735,\n",
       " 0.2755065870853869,\n",
       " 0.29599941047596834,\n",
       " 0.2097619770067749,\n",
       " 0.14532434041400166,\n",
       " 0.140381186147014,\n",
       " 0.12231373711598903,\n",
       " 0.16023928961292355,\n",
       " 0.20792777297563347,\n",
       " 0.20552802600960873,\n",
       " 0.2675945689844913,\n",
       " 0.23461849854112304,\n",
       " 0.22597290470846265,\n",
       " 0.20722709165609132,\n",
       " 0.1629839166072391,\n",
       " 0.19733555890959098,\n",
       " 0.3335471452962162,\n",
       " 0.2857059612847127,\n",
       " 0.10895690574082106,\n",
       " 0.1818210582101838,\n",
       " 0.18686504798622305,\n",
       " 0.173310012013291,\n",
       " 0.2042352673272878,\n",
       " 0.23490998616358783,\n",
       " 0.3441065099711753,\n",
       " 0.20219025982585737,\n",
       " 0.18955899433621504,\n",
       " 0.19173894852138573,\n",
       " 0.334607181653483,\n",
       " 0.25000261674263335,\n",
       " 0.11967084716649634,\n",
       " 0.26592684746748274,\n",
       " 0.12923388557856108,\n",
       " 0.17940150791366705,\n",
       " 0.11505155658113275,\n",
       " 0.228658421221702,\n",
       " 0.17718776650472556,\n",
       " 0.1966862016035712,\n",
       " 0.15671541501482034,\n",
       " 0.25096129791544164,\n",
       " 0.17456244146339817,\n",
       " 0.18098203688115588,\n",
       " 0.22986263896111558,\n",
       " 0.25678623635683906,\n",
       " 0.18600640761474396,\n",
       " 0.092755130361871,\n",
       " 0.23265083961066224,\n",
       " 0.27994576466590093,\n",
       " 0.2210200610214711,\n",
       " 0.252204645590529,\n",
       " 0.2221358063657417,\n",
       " 0.19111404173829816,\n",
       " 0.20284529799960105,\n",
       " 0.2254121518523211,\n",
       " 0.206458263355072,\n",
       " 0.22464470801363245,\n",
       " 0.24382657692814913,\n",
       " 0.21189336062165798,\n",
       " 0.21746880907258725,\n",
       " 0.2296196483083328,\n",
       " 0.16346779040190074,\n",
       " 0.16312458489909343,\n",
       " 0.17249149680487563,\n",
       " 0.27468948330488085,\n",
       " 0.19432439899962795,\n",
       " 0.16605035396313006,\n",
       " 0.2178863138965742,\n",
       " 0.18556045128239262,\n",
       " 0.16399908658718132,\n",
       " 0.23782771108202783,\n",
       " 0.2333106211692194,\n",
       " 0.24333531774532458,\n",
       " 0.2901449730472754,\n",
       " 0.2278885767628802,\n",
       " 0.21064336468085254,\n",
       " 0.22833757567789614,\n",
       " 0.1271528738248522,\n",
       " 0.1580827770132196,\n",
       " 0.15516545331594422,\n",
       " 0.21691519876616605,\n",
       " 0.16641227783950763,\n",
       " 0.1898602682347721,\n",
       " 0.2275843004241348,\n",
       " 0.30481395115960597,\n",
       " 0.25192457257989775,\n",
       " 0.19749636320524622,\n",
       " 0.18250922545184178,\n",
       " 0.1969982231164279,\n",
       " 0.1279346727656442,\n",
       " 0.22410768803485415,\n",
       " 0.22222205110219462,\n",
       " 0.1803524553272996,\n",
       " 0.15554569198722734,\n",
       " 0.16134046118410017,\n",
       " 0.16372320202407242,\n",
       " 0.18796559074401475,\n",
       " 0.11797358282716786,\n",
       " 0.18538158585629994,\n",
       " 0.17190941750805389,\n",
       " 0.13259255644388313,\n",
       " 0.19994456510622616,\n",
       " 0.27627739055177813,\n",
       " 0.1785529809582258,\n",
       " 0.2747401741330112,\n",
       " 0.2469020950412676,\n",
       " 0.1687743579377985,\n",
       " 0.18988817350954146,\n",
       " 0.17269199196151597,\n",
       " 0.1511526396957853,\n",
       " 0.24042719397823165,\n",
       " 0.20341928074355453,\n",
       " 0.12384219305673616,\n",
       " 0.19441484744239546,\n",
       " 0.20183901165472518,\n",
       " 0.17117950149274597,\n",
       " 0.1591973867828712,\n",
       " 0.19344124650714134,\n",
       " 0.22080315708403642,\n",
       " 0.15870414050002712,\n",
       " 0.2560456702450854,\n",
       " 0.2380791279550466,\n",
       " 0.24129774162633777,\n",
       " 0.310500230395112,\n",
       " 0.1724509447718494,\n",
       " 0.16621808745791855,\n",
       " 0.21545713906518638,\n",
       " 0.3385316960028447,\n",
       " 0.19393073949235096,\n",
       " 0.2281631067484433,\n",
       " 0.221336490891469,\n",
       " 0.12722409985096783,\n",
       " 0.13998789064634298,\n",
       " 0.19353702150616292,\n",
       " 0.20104225934135234,\n",
       " 0.3342396070722281,\n",
       " 0.16787797411133876,\n",
       " 0.2134946086730405,\n",
       " 0.23076504053704538,\n",
       " 0.12626550218080557,\n",
       " 0.2039125165607817,\n",
       " 0.2612324755182075,\n",
       " 0.17379617557736537,\n",
       " 0.21713455752024502,\n",
       " 0.22021582745238433,\n",
       " 0.22527991406860048,\n",
       " 0.30294280234533155,\n",
       " 0.17382783001491375,\n",
       " 0.21944251927233793,\n",
       " 0.22545911375079208,\n",
       " 0.1927315684051354,\n",
       " 0.29002205567383077,\n",
       " 0.20212009918222473,\n",
       " 0.3363629955548374,\n",
       " 0.18238104832059754,\n",
       " 0.27844875917569223,\n",
       " 0.21364230593991984,\n",
       " 0.17614852798285596,\n",
       " 0.19443089344362785,\n",
       " 0.26838562866066895,\n",
       " 0.24315865100145084,\n",
       " 0.18823120582957287,\n",
       " 0.16053568868782142,\n",
       " 0.11587515539527282,\n",
       " 0.3159647348491199,\n",
       " 0.15149743855399422,\n",
       " 0.26942962218433075,\n",
       " 0.19535920718232214,\n",
       " 0.21688161280562457,\n",
       " 0.2036435214990555,\n",
       " 0.20840128029138827,\n",
       " 0.17636605344995904,\n",
       " 0.2760234694049758,\n",
       " 0.11663392585497467,\n",
       " 0.18883364356103471,\n",
       " 0.37704627886060804,\n",
       " 0.23259494813173298,\n",
       " 0.1831567684758003,\n",
       " 0.21837169461513561,\n",
       " 0.14501200432471528,\n",
       " 0.2247460123460686,\n",
       " 0.17274028298485378,\n",
       " 0.26046916830439903,\n",
       " 0.2178870210863333,\n",
       " 0.21939314122426556,\n",
       " 0.2118679910945146,\n",
       " 0.25085037037854774,\n",
       " 0.2232121012357131,\n",
       " 0.22042880860403058]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.018\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     3.398\n",
      "Date:                Thu, 17 Oct 2024   Prob (F-statistic):             0.0669\n",
      "Time:                        13:28:59   Log-Likelihood:                -194.25\n",
      "No. Observations:                 183   AIC:                             392.5\n",
      "Df Residuals:                     181   BIC:                             398.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.3777      0.306      4.495      0.000       0.773       1.982\n",
      "x1             0.0616      0.033      1.843      0.067      -0.004       0.128\n",
      "==============================================================================\n",
      "Omnibus:                        3.753   Durbin-Watson:                   1.887\n",
      "Prob(Omnibus):                  0.153   Jarque-Bera (JB):                3.675\n",
      "Skew:                           0.300   Prob(JB):                        0.159\n",
      "Kurtosis:                       2.651   Cond. No.                         54.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "Y = df[\"difficulty_aggregrate\"].to_numpy()\n",
    "X = df[[\"readability\"]].to_numpy()\n",
    "\n",
    "# [[\"perplexity\",\"burstiness\", \"fano_factor\", \"ARI\", \"ttr\"]]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIG-CUDA-12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
